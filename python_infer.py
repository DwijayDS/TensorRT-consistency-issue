import json
import subprocess
from typing import List, Union

import pycuda.autoinit
import pycuda.driver as cuda
import tensorrt as trt

logger = trt.Logger(trt.Logger.WARNING)
import numpy as np


class TensorrtInfer():

    def load_model(self, engine_file):
        """Load TensorRt engine
        """
        runtime = trt.Runtime(logger)
        with open(engine_file, 'rb') as f:
            serialized_engine = f.read()
        engine = runtime.deserialize_cuda_engine(serialized_engine)
        self.context = engine.create_execution_context()
        self.inputs, self.outputs, self.bindings = [], [], []
        self.stream = cuda.Stream()
        self.input_names = [binding for binding in engine if engine.binding_is_input(binding)]
        self.output_names = [binding for binding in engine if not engine.binding_is_input(binding)]
        for binding in self.input_names + self.output_names:
            size = trt.volume(engine.get_binding_shape(binding))
            dtype = trt.nptype(engine.get_binding_dtype(binding))
            # Allocate host and device buffers
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            # Append the device buffer to device bindings.
            self.bindings.append(int(device_mem))
            # Append to the appropriate list.
            if engine.binding_is_input(binding):
                self.inputs.append(HostDeviceMem(host_mem, device_mem))
            else:
                self.outputs.append(HostDeviceMem(host_mem, device_mem))

    def forward(self, inputs: Union[List[np.ndarray], np.ndarray], output_shapes) -> Union[List[np.ndarray], np.ndarray]:
        """Perform Inference using Tensor RT graph.

        Args:
            inputs (Union[List[np.ndarray], np.ndarray]): Numpy array or a List of numpy array.

        Returns:
            Union[List[np.ndarray], np.ndarray]: Output generated by the Tensor RT model.
        """
        inputs = [inputs] if not isinstance(inputs, list) else inputs
        # change the input holder's memory pointer to the specific numpy array's address
        for inp_holder, inp_data in zip(self.inputs, inputs):
            inp_holder.host = inp_data
        # Transfer input data to the GPU.
        [cuda.memcpy_htod_async(inp.device, inp.host, self.stream) for inp in self.inputs]
        # Run inference.
        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)
        # Transfer predictions back from the GPU.
        [cuda.memcpy_dtoh_async(out.host, out.device, self.stream) for out in self.outputs]
        # Synchronize the stream
        self.stream.synchronize()
        # Return only the host outputs.
        outputs = [out.host for out in self.outputs]
        # reshape the outputs
        outputs = [np.reshape(out, shape) for out, shape in zip(outputs, output_shapes)]
        return {name:out for name, out in zip(self.output_names,outputs)}

class HostDeviceMem:

    def __init__(self, host_mem: np.ndarray, device_mem: cuda.DeviceAllocation):
        """Assigning host parameters for TensorRT execution.

        Args:
            host_mem (np.ndarray): Numpy array containing memory information of host  machine
            device_mem (cuda.DeviceAllocation): Device Allocation object containing memory information of GPU.
        """
        self.host = host_mem
        self.device = device_mem

    def __str__(self):
        return "Host:\n" + str(self.host) + "\nDevice:\n" + str(self.device)

    def __repr__(self):
        return self.__str__()

def compare_outputs(np_dict, trt_dict):
    for i in range(len(trt_dict)):
        name = trt_dict[i]['name']
        values_np = np.squeeze(np_dict[name])
        values_trt = np.asarray(prediction_trt[i]['values']).reshape(values_np.shape)
        print('*************************************************************')
        print('Prediction name is ', name)
        print('Max Prediction difference is : ', np.max(np.abs(values_np-values_trt)))
        print('Mean Prediction difference is: ', np.mean(np.abs(values_np-values_trt)))

if __name__ == "__main__":

    engine_file = './model.trt'
    inp_path = './data.npy'
    load_inp = np.load(inp_path)
    output_shapes = [[1, 64, 64, 17], [1, 64, 64, 17]]

    # Infer TRT model using python api
    inf = TensorrtInfer()
    inf.load_model(engine_file)
    python_api_out = inf.forward(load_inp, output_shapes)

    # Infer TRT model using trtexec command line
    # save numpy array to binary file
    binary_data_path = './data.dat'
    load_inp.tofile(binary_data_path)
    output_path = './trtexec_out.json'
    command = "trtexec --loadEngine={} --loadInputs='input_1:{}' --exportOutput={}".format(engine_file,binary_data_path,output_path)
    # run command
    process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)

    # load trtexec json output
    with open(output_path) as json_file:
        prediction_trt = json.load(json_file)

    print('*********************************TRT API VS TRTEXEC***************************************')
    compare_outputs(python_api_out,prediction_trt)